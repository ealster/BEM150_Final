BEM150 Final Project
========================================================
Eli Alster  
Vansh Kumar  
Angad Rekhi  

     We analyzed the NYC traffic data set. In order to simplify analysis, we only considered the 2013 data so the data set would be fixed.
  
```{r LoadData, echo=FALSE, cache=FALSE}
     library("plyr")
     library("hexbin")
     #install.packages('e1071', dependencies = TRUE)
     library(e1071)
     
     # Read raw data
     df_main = read.csv("collisions.csv", header = TRUE, sep="\t")

     # Extract training and test sets and replace NA's with 0
     df_main[is.na(df_main)] = 0
     df_used = subset(df_main, (year == 2012 | year == 2013))

     df_used$lat = as.numeric(as.character(df_used$lat))
```

```{r fig.width=7, fig.height=6}
     scooter_accidents = subset(df, scooter == TRUE)
     plot(scooter_accidents$lon, scooter_accidents$lat)
```
Plot of bicycle accidents.

```{r}
     df_in = subset(df, lat < 44)
     bin = hexbin(df_in$lon, df_in$lat, xbi=75)
     plot(bin, main="2013 Traffic Deaths in NY by Location", xlab="Longitude", ylab="")
     mtext(text = "Latitude", side=2, line=3)
```
Accident frequency map!

Histogram of reason for accident vs total people killed.

```{r Reasons_of_Death}
     reasons = df[,39:68]

     mat = matrix(nrow=4, ncol=ncol(reasons))
     for ( r in 1:ncol(reasons) ) 
     {
          reason = names(reasons)[[r]]
          reason_row = subset(df, df[[reason]]==TRUE)
          ped = sum(reason_row[,"pedestr_killed"])
          pass = sum(reason_row[,"passengers_killed"])
          motor = sum(reason_row[,"motorists_killed"])
          cycle = sum(reason_row[,"cyclists_killed"])
          
          mat[,r] = c(ped, pass, motor, cycle)
     }

     par(mar=c(4,10,2,1)) # increase margins on plot 
     par(las=2)          # align accident names horizontally
     colors=c("blue","green","red","black")
     barplot(mat, horiz=TRUE, names.arg=colnames(reasons), col=colors, main="2013 NYC Deaths by Kind of Accident", xlab="Number of Deaths", xlim=c(0,40), cex.names=0.5)
     legend(x=25,y=25,fill=colors,legend=c("Pedestrians","Passengers","Motorists","Cyclists"))
 
```

``` {r Data_Munging}

# Notes:
# x = classification matrix
# y = factors for each type
# scale = FALSE if just factors
# 39:68 reason, lon, lat, 22:38 vehicle type

# Separate training and test sets
df_training = subset(df_used, year == 2012)
df_test = subset(df_used, year == 2013)

# Training Set
#    Extract explanatory variables
training_x = data.frame(df_training$lon, df_training$lat, df_training[,39:68], df_training[,22:38])
#    Extract dependent variable
training_y = as.numeric(df_training$total_killed != 0)  # note: vector, not data frame

# Test Set
#    Extract explanatory variables
test_x = data.frame(df_test$lon, df_test$lat, df_test[,39:68], df_test[,22:38])
#    Extract dependent variable
test_y = as.numeric(df_test$total_killed != 0)  # note: vector, not data frame

# Train SVM
model = svm(x = training_x, y = as.factor(training_y), kernel = "linear", cost = .1, type = "C-classification", scale = FALSE)

# Test SVM
predictions = predict(model, newdata = data.frame(x = training_x), decision.values = TRUE)

# Check prediction accuracy
print(sum(test_y == predictions) / nrow(test_x))

# # Testing
# spam_test = read.csv('Spam_TestSet.csv')
# answers = spam_test[,1]
# predictions = predict(model, spam_test[,-1], decision.values = TRUE)
# 
# # See how many were correct
# trials = nrow(spam_test)
# correct_amt = sum(answers == predictions)
# correct_pct = correct_amt / trials
# 
# # Calculated most important words
# var_weights = t(model[["coefs"]]) %*% model[["SV"]]
# vocab_list = read.csv('vocab.txt', sep="\t", header = FALSE)

```

